{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10382284,"sourceType":"datasetVersion","datasetId":6431556},{"sourceId":10462098,"sourceType":"datasetVersion","datasetId":6477176},{"sourceId":10693271,"sourceType":"datasetVersion","datasetId":6625827},{"sourceId":10698172,"sourceType":"datasetVersion","datasetId":6629554}],"dockerImageVersionId":30822,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\n# Step 1: Load the Excel file\nfile_path = \"/kaggle/input/gse18520/GSE18520.xlsx\"  # Replace with your actual file path\ndata = pd.read_excel(file_path)\n\n# Step 2: Save the data to a CSV file\ncsv_file_path = \"GSE18520.csv\"  # Specify the output CSV file path\ndata.to_csv(csv_file_path, index=False)  # Save without the index\n\nprint(f\"File converted to CSV and saved as {csv_file_path}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-07T18:58:58.058164Z","iopub.execute_input":"2025-02-07T18:58:58.058574Z","iopub.status.idle":"2025-02-07T18:59:06.931072Z","shell.execute_reply.started":"2025-02-07T18:58:58.058537Z","shell.execute_reply":"2025-02-07T18:59:06.929940Z"}},"outputs":[{"name":"stdout","text":"File converted to CSV and saved as GSE18520.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\n\n# Step 1: Load the CSV file\nfile_path = \"/kaggle/working/GSE18520.csv\"  # Replace with your file path\ndata = pd.read_csv(file_path)\n\n# Step 2: Keep only the specified columns\ncolumns_to_keep = ['ID', 'adj.P.Val', 'P.Value', 'logFC', 'Gene.symbol']\nfiltered_data = data[columns_to_keep]\n\n# Step 3: Filter the data based on conditions\n# Upregulated genes\nupregulated_gene = filtered_data[(filtered_data['adj.P.Val'] < 0.05) & (filtered_data['logFC'] > 1)]\n\n# Downregulated genes\ndownregulated_gene = filtered_data[(filtered_data['adj.P.Val'] < 0.05) & (filtered_data['logFC'] < -1)]\n\n# All data (already filtered to include only the specified columns)\nall_data = filtered_data\n\n# Step 4: Save the filtered data\nupregulated_gene.to_csv('GSE18520_upregulated_gene.csv', index=False)\ndownregulated_gene.to_csv('GSE18520_downregulated_gene.csv', index=False)\nall_data.to_csv('GSE18520_all_genes.csv', index=False)\n\nprint(\"Files created successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T18:32:01.903205Z","iopub.execute_input":"2025-01-16T18:32:01.903581Z","iopub.status.idle":"2025-01-16T18:32:02.249857Z","shell.execute_reply.started":"2025-01-16T18:32:01.903551Z","shell.execute_reply":"2025-01-16T18:32:02.248765Z"}},"outputs":[{"name":"stdout","text":"Files created successfully!\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import pandas as pd\n\n# Load the CSV file\nfile_path = '/kaggle/working/GSE18520_upregulated_gene.csv'  # Replace with your file path\ndf = pd.read_csv(file_path)\n\n# Check column names\nprint(df.columns)\n\n# Ensure 'Gene.symbol' is correctly referenced, handle potential whitespace\ndf.columns = df.columns.str.strip()  # Strip any whitespace from column names\n\n# Filter out rows where 'Gene.symbol' is null, 'nan', or 'NAN', if 'Gene.symbol' exists\nif 'Gene.symbol' in df.columns:\n    # Remove rows with NaN or 'nan'/'NAN' in 'Gene.symbol'\n    filtered_df = df[df['Gene.symbol'].notna() & ~(df['Gene.symbol'].str.upper() == 'NAN')]\n\n    # Ensure 'P.Value' column exists and remove duplicates based on 'Gene.symbol', keeping the highest 'P.Value'\n    if 'P.Value' in filtered_df.columns:  # Replace 'P.Value' with the actual p-value column name if different\n        filtered_df = filtered_df.loc[filtered_df.groupby('Gene.symbol')['P.Value'].idxmax()]\n        \n        # Save the filtered DataFrame to a new CSV file\n        filtered_df.to_csv('filtered_upregulated_GSE18520.csv', index=False)\n        print(\"Rows with null, 'NAN' or 'nan' values in 'Gene.symbol' have been filtered out, and duplicates were removed based on the highest p-value.\")\n    else:\n        print(\"Column 'P.Value' not found in the CSV file.\")\nelse:\n    print(\"Column 'Gene.symbol' not found in the CSV file.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T18:33:44.144703Z","iopub.execute_input":"2025-01-16T18:33:44.145057Z","iopub.status.idle":"2025-01-16T18:33:44.315864Z","shell.execute_reply.started":"2025-01-16T18:33:44.145033Z","shell.execute_reply":"2025-01-16T18:33:44.314833Z"}},"outputs":[{"name":"stdout","text":"Index(['ID', 'adj.P.Val', 'P.Value', 'logFC', 'Gene.symbol'], dtype='object')\nRows with null, 'NAN' or 'nan' values in 'Gene.symbol' have been filtered out, and duplicates were removed based on the highest p-value.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import pandas as pd\n\n# Define input file names\nfile1 = \"/kaggle/input/filtered-deg/filtered_downregulated_GSE26712.csv\"  # Change to your actual file name\nfile2 = \"/kaggle/input/filtered-deg/filtered_upregulated_GSE26712.csv\"\n\n# Define output file name\noutput_file = \"combined_DEG_Filtered_GSE26712.csv\"\n\n# Read the CSV files\ndf1 = pd.read_csv(file1)\ndf2 = pd.read_csv(file2)\n\n# Combine the two dataframes by stacking rows\ncombined_df = pd.concat([df1, df2], ignore_index=True)\n\n# Save the combined data to a new CSV file\ncombined_df.to_csv(output_file, index=False)\n\nprint(f\"Combined CSV saved as {output_file}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T07:28:44.973576Z","iopub.execute_input":"2025-02-08T07:28:44.973926Z","iopub.status.idle":"2025-02-08T07:28:45.005542Z","shell.execute_reply.started":"2025-02-08T07:28:44.973901Z","shell.execute_reply":"2025-02-08T07:28:45.004435Z"}},"outputs":[{"name":"stdout","text":"Combined CSV saved as combined_DEG_Filtered_GSE26712.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Load CSV file\ninput_file = \"/kaggle/working/combined_DEG_Filtered_GSE26712.csv\"  # Change this to your CSV file path\noutput_file = \"combined_DEG_Filtered_GSE26712_log2.csv\"\n\n# Read the CSV file\ndf = pd.read_csv(input_file)\n\n# Apply log2 transformation to numeric columns (avoiding non-numeric ones)\ndf_log2 = df.applymap(lambda x: np.log2(x) if np.issubdtype(type(x), np.number) and x > 0 else x)\n\n# Save transformed data to a new CSV file\ndf_log2.to_csv(output_file, index=False)\n\nprint(f\"Log2 transformed data saved to {output_file}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T07:30:51.948310Z","iopub.execute_input":"2025-02-08T07:30:51.948645Z","iopub.status.idle":"2025-02-08T07:30:51.988269Z","shell.execute_reply.started":"2025-02-08T07:30:51.948620Z","shell.execute_reply":"2025-02-08T07:30:51.987236Z"}},"outputs":[{"name":"stdout","text":"Log2 transformed data saved to combined_DEG_Filtered_GSE26712_log2.csv\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-4-cbd906c0fdc1>:12: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n  df_log2 = df.applymap(lambda x: np.log2(x) if np.issubdtype(type(x), np.number) and x > 0 else x)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import quantile_transform\nimport os\n\n# Step 1: Specify the input and output file paths\ninput_file = '/kaggle/working/combined_DEG_Filtered_GSE18520.csv'  # Replace with your input CSV file\noutput_file = 'GSE18520_log_quantile_normalized_data.csv'  # Output CSV file\n\n# Step 2: Check if the input file exists\nif not os.path.exists(input_file):\n    raise FileNotFoundError(f\"Input file '{input_file}' not found. Please check the path.\")\n\n# Step 3: Read the microarray dataset from the CSV file\nmicroarray_data = pd.read_csv(input_file)\n\n# Ensure the \"ID_REF\" column exists\nif \"ID_REF\" not in microarray_data.columns:\n    raise ValueError(\"'ID_REF' column not found in the dataset. Ensure the dataset has this column.\")\n\n# Set the \"ID_REF\" column as the index\nmicroarray_data.set_index(\"ID_REF\", inplace=True)\n\n# Step 4: Handle non-positive values before log2 transformation\nif (microarray_data <= 0).any().any():\n    print(\"Warning: Dataset contains non-positive values. Adding a small constant (1e-5) to the data.\")\n    microarray_data += 1e-5  # Make all values strictly positive\n\n# Apply log2 transformation\nlog2_transformed_data = np.log2(microarray_data)\n\n# Step 5: Perform quantile normalization on the log2-transformed data\nquantile_normalized_data = quantile_transform(\n    log2_transformed_data, \n    n_quantiles=min(50, log2_transformed_data.shape[0]),  # Adjust n_quantiles to data size\n    axis=0, \n    random_state=0, \n    copy=True\n)\n\n# Step 6: Convert the normalized data back to a DataFrame\nquantile_normalized_df = pd.DataFrame(\n    quantile_normalized_data, \n    columns=microarray_data.columns, \n    index=microarray_data.index\n)\n\n# Step 7: Write the normalized data to a new CSV file\nquantile_normalized_df.to_csv(output_file)\n\nprint(f\"Normalized data has been saved to '{output_file}'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T08:03:22.874326Z","iopub.execute_input":"2025-02-08T08:03:22.874659Z","iopub.status.idle":"2025-02-08T08:03:23.477030Z","shell.execute_reply.started":"2025-02-08T08:03:22.874634Z","shell.execute_reply":"2025-02-08T08:03:23.475646Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-046004a3e704>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Ensure the \"ID_REF\" column exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"ID_REF\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmicroarray_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'ID_REF' column not found in the dataset. Ensure the dataset has this column.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Set the \"ID_REF\" column as the index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: 'ID_REF' column not found in the dataset. Ensure the dataset has this column."],"ename":"ValueError","evalue":"'ID_REF' column not found in the dataset. Ensure the dataset has this column.","output_type":"error"}],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\n\n# Step 1: Load the CSV file\ninput_file = '/kaggle/input/1852000/merged_GSE18520_fresh.csv'  # Replace with your actual file name\noutput_file = 'merged_GSE18520_fresh_with_Class.csv'\n\ndf = pd.read_csv(input_file, index_col=\"ID_REF\")  # Set ID_REF as index\n\n# Step 2: Create the \"Class\" row\nclass_values = []  # List to store class values\n\nfor column in df.columns:\n    if 'GSM462643' <= column <= 'GSM462652':  # Check if column falls in range\n        class_values.append(0)\n    else:\n        class_values.append(1)\n\n# Step 3: Append the new \"Class\" row at the bottom\ndf.loc[\"Class\"] = class_values\n\n# Step 4: Save the modified DataFrame to a new CSV file\ndf.to_csv(output_file)\n\nprint(f\"Updated CSV file saved as '{output_file}'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T19:34:15.743588Z","iopub.execute_input":"2025-02-08T19:34:15.743983Z","iopub.status.idle":"2025-02-08T19:34:16.258759Z","shell.execute_reply.started":"2025-02-08T19:34:15.743957Z","shell.execute_reply":"2025-02-08T19:34:16.257416Z"}},"outputs":[{"name":"stdout","text":"Updated CSV file saved as 'merged_GSE18520_fresh_with_Class.csv'\n","output_type":"stream"}],"execution_count":2}]}